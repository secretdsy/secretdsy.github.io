---
title: "[논문 리뷰] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
date: 2025-04-16 14:00:00 +0900
categories: [Paper]
tags: [Transformer, Vision, ViT]
math: true
toc: true
---

## 📝 논문 정보

- **제목**: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale  
- **저자**: Dosovitskiy et al.  
- **학회/연도**: ICLR 2021  
- **링크**: [https://arxiv.org/pdf/2010.11929](https://arxiv.org/pdf/2010.11929)

---

## 1. Introduction

- prepend class token
- 2d PE
- Vision Transformer has much less image-specific inductive bias than CNNs
- patch size가 작을수록 sequence length는 커짐 -> 연산량 : ViT-16 > ViT-32
- 데이터셋이 얼마나 큰 영향을 미칠까? 
    - 큰 데이터셋에서는 vit가 더 좋은 성능보임


---

## 🔗 Reference
[1] [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/pdf/2010.11929)