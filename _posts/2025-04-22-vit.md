---
title: "[논문 리뷰] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (ViT)"
date: 2025-04-22 14:00:00 +0900
categories: [Paper]
tags: [Transformer, Vision, ViT]
math: true
toc: true
---

## 📝 논문 정보

- **제목**: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale  
- **저자**: Dosovitskiy et al.  
- **학회/연도**: ICLR 2021  
- **링크**: [https://arxiv.org/pdf/2010.11929](https://arxiv.org/pdf/2010.11929)

---

## 1. Introduction

Transformer의 출시, 그리고 그 이후 관련 연구로 인해 Language 분야는 많은 발전을 이뤄왔다. 하지만 Vision 분야는 기존의 CNN구조를 벗어나지 못한 채 꽤 오랜 기간 정체되어있었다.  
이 논문은 CNN구조를 완전히 벗어난 Vision-Transformer(ViT)로 불리는 Transformer 기반 분류 방법을 제시했다. 이미지를 패치 단위로 쪼개서 각 패치를 단어, 이미지 전체를 문장인 것처럼 여기도록 접근했으며, Language 분야에서 높은 성능을 낸 것처럼 Vision 분야에서도 좋은 성능을 보이고 있다.  
이어서 CNN과의 차이, 왜 Transformer 구조가 성능이 잘 나오는지, Transformer 구조 덕분에 분류 성능 말고도 어떠한 장점이 있는지에 대해 설명하고자 한다.  

Transformer에 대한 자세한 내용은 이전에 리뷰한 내용을 먼저 보고 오는 것을 추천한다.  
[https://secretdsy.github.io/paper/transformer/](https://secretdsy.github.io/paper/transformer/)  

---

## 2. CNN vs Transformer

**이미지 분류에서 CNN이 잘 작동하는 이유는 뭘까?**  
CNN을 활용한 분류 과정을 살펴보면 다음과 같은 방식으로 작동한다.  

[CNN 구조 이미지]  
입력 -> **특징 추출을 위한 컨볼루션 연산** -> FC Layer -> 클래스 분류  

[feature map projection 이미지]  
**특징 추출을 위한 컨볼루션 연산** 은 입력층에 가까울 수록 좁은 구역을 보고, 출력층으로 갈수록 더 넓은 영역을 보도록 설계되어있다.  
예를 들어 (224 * 224 * 3) 이미지를 입력으로하고 컨볼루션 필터 사이즈가 (3 * 3) 일 때, 첫 번째 레이어의 필터는 가로, 세로 3픽셀씩만 보게 된다. 그리고 두번째 레이어의 입력은 첫번째 레이어에서 특징을 추출한 것이고 보통은 가로, 세로 사이즈가 더 작아진다.  
이렇게 작아진 입력에 똑같은 사이즈의 필터를 적용하면 원본에서 3픽셀이 아닌 더 많은 영역을 보는 것과 같다. 따라서 CNN은 레이어가 깊어질수록 더 넓은 영역을 보는 것처럼 작동한다.  

이러한 방식으로 인해 CNN은 "edge" 부분에 집중하고, 이후 레이어에서는  "texture 등 더 넓은 영역"에 집중하고, 최종적으로 "object" 전체를 보는 것과 같이 **계층적 구조(Hierarchical Structure)** 로 이루어져 있다.  

그리고 컨볼루션 연산은 **똑같은 필터(Weight Sharing)** 를 슬라이딩 윈도우 방식으로 이미지 전체를 훑어보고, 컨볼루션 연산 이후에 max pooling 또는 average pooling을 적용하기 때문에 픽셀이 이동하더라도 비슷한 특징을 추출한다. 이러한 방식 덕분에 CNN은 **이동 불변성(Translation Equivariance)** 이라는 특징을 갖게 된다.

또한, Transformer는 self-attention 연산을 통해 **"입력 전체"** 를 살펴보고 어느 부분을 더 **"Attend"** 할지를 정하도록 되어있다. 따라서 (N * N) 형태로 **국소적인 영역(Locality)** 을 훑고 다니는 컨볼루션 연산과 다르게 전체를 본다는 점이 가장 큰 차이다.  

앞에서 설명한 **"Translation Equivariance, Hierarchical Structure, Locality"** 로 인해 CNN은 Transformer에 비해 **Inductive Bias** 가 강하다고 표현한다.  

Transformer는 CNN에 비해 **Inductive Bias** 가 약한데 어떻게 성능을 더 높일 수 있었던건지, **Inductive Bias** 가 약해서 얻는 이점이 어떤건지는 이후에 설명하려한다.

---

## 3. ViT Architecture

**이미지를 어떻게 입력받을까?**  
Transformer는 문장 전체, 즉 단어들을 입력받는다. ViT는 이미지를 여러 패치로 쪼개고, 각각의 패치를 단어처럼 취급하여 입력받는다.  

패치의 개수는 다음과 같이 구하고, 이 논문에서는 패치사이즈 $P = 16, 32$ 를 사용했다.  
$$
N = \frac{H * W}{P^2}
$$  

예를 들어 이미지 사이즈가 (224 * 224)이고, $P = 16$ 이라면 $N = 196$ 이 되고, 이미지 분류를 위해 [CLS] 토큰을 패치의 가장 앞에 추가한다. 이 토큰은 랜덤한 값으로 초기화하고, 이미지의 전체 정보를 요약하여 최종 분류에 사용한다.  
또한, Transformer와 동일하게 Positional Embedding(PE)까지 더해주고나면 최종 입력이 된다.  



---

## 🔗 Reference
[1] [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/pdf/2010.11929)