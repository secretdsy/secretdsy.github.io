---
title: "[논문 리뷰] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (ViT)"
date: 2025-04-22 14:00:00 +0900
categories: [Paper]
tags: [Transformer, Vision, ViT]
math: true
toc: true
---

## 📝 논문 정보

- **제목**: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale  
- **저자**: Dosovitskiy et al.  
- **학회/연도**: ICLR 2021  
- **링크**: [https://arxiv.org/pdf/2010.11929](https://arxiv.org/pdf/2010.11929)

---

## 1. Introduction

Transformer의 출시, 그리고 그 이후 관련 연구로 인해 Language 분야는 많은 발전을 이뤄왔다. 하지만 Vision 분야는 기존의 CNN구조를 벗어나지 못한 채 꽤 오랜 기간 정체되어있었다.  
이 논문은 CNN구조를 완전히 벗어난 Vision-Transformer(ViT)로 불리는 Transformer 기반 분류 방법을 제시했다. 이미지를 패치 단위로 쪼개서 각 패치를 단어, 이미지 전체를 문장인 것처럼 여기도록 접근했으며, Language 분야에서 높은 성능을 낸 것처럼 Vision 분야에서도 좋은 성능을 보이고 있다.  
이어서 CNN과의 차이, 왜 Transformer 구조가 성능이 잘 나오는지, Transformer 구조 덕분에 분류 성능 말고도 어떠한 장점이 있는지에 대해 설명하고자 한다.  

Transformer에 대한 자세한 내용은 이전에 리뷰한 내용을 먼저 보고 오는 것을 추천한다.  
[https://secretdsy.github.io/paper/transformer/](https://secretdsy.github.io/paper/transformer/)  

---

## 2. CNN vs Transformer

**이미지 분류에서 CNN이 잘 작동하는 이유는 뭘까?**  
CNN을 활용한 분류 과정을 살펴보면 다음과 같은 방식으로 작동한다.  

![CNN-architecture](/assets/2025-04-22-vit/1.png)  
_(CNN의 시초인 LeNet의 구조)_  

입력 -> **특징 추출을 위한 컨볼루션 연산** -> FC Layer -> 클래스 분류  

**특징 추출을 위한 컨볼루션 연산** 은 입력층에 가까울 수록 좁은 구역을 보고, 출력층으로 갈수록 더 넓은 영역을 보도록 설계되어있다.  
예를 들어 (224 * 224 * 3) 이미지를 입력으로하고 컨볼루션 필터 사이즈가 (3 * 3) 일 때, 첫 번째 레이어의 필터는 가로, 세로 3픽셀씩만 보게 된다. 그리고 두번째 레이어의 입력은 첫번째 레이어에서 특징을 추출한 것이고 보통은 가로, 세로 사이즈가 더 작아진다.  

![CNN-projection](/assets/2025-04-22-vit/1-2.png)  
_(두번째 컨볼루션 연산에서 얻은 feature map에서의 한 픽셀이 입력에서 차지하는 부분)_  

이렇게 작아진 입력에 똑같은 사이즈의 필터를 적용하면 원본에서 3픽셀이 아닌 더 많은 영역을 보는 것과 같다. 따라서 CNN은 레이어가 깊어질수록 더 넓은 영역을 보는 것처럼 작동한다.  

이러한 방식으로 인해 CNN은 "edge" 부분에 집중하고, 이후 레이어에서는  "texture 등 더 넓은 영역"에 집중하고, 최종적으로 "object" 전체를 보는 것과 같이 **계층적 구조(Hierarchical Structure)** 로 이루어져 있다.  

그리고 컨볼루션 연산은 **똑같은 필터(Weight Sharing)** 를 슬라이딩 윈도우 방식으로 이미지 전체를 훑어보고, 컨볼루션 연산 이후에 max pooling 또는 average pooling을 적용하기 때문에 픽셀이 이동하더라도 비슷한 특징을 추출한다. 이러한 방식 덕분에 CNN은 **이동 불변성(Translation Equivariance)** 이라는 특징을 갖게 된다.

또한, Transformer는 self-attention 연산을 통해 **"입력 전체"** 를 살펴보고 어느 부분을 더 **"Attend"** 할지를 정하도록 되어있다. 따라서 (N * N) 형태로 **국소적인 영역(Locality)** 을 훑고 다니는 컨볼루션 연산과 다르게 전체를 본다는 점이 가장 큰 차이다.  

앞에서 설명한 **"Translation Equivariance, Hierarchical Structure, Locality"** 로 인해 CNN은 Transformer에 비해 **Inductive Bias** 가 강하다고 표현한다.  

Transformer는 CNN에 비해 **Inductive Bias** 가 약한데 어떻게 성능을 더 높일 수 있었던건지, **Inductive Bias** 가 약해서 얻는 이점이 어떤건지는 이후에 설명하려한다.

---

## 3. ViT Architecture

![ViT-architecture](/assets/2025-04-22-vit/2.png)  
_(왼쪽: ViT 구조, 오른쪽: Transformer Encoder)_  

**이미지를 어떻게 입력받을까?**  
Transformer는 문장 전체, 즉 단어들을 입력받는다. ViT는 이미지를 여러 패치로 쪼개고, 각각의 패치를 단어처럼 취급하여 입력받는다.  

이 논문에서는 패치사이즈 $P = 16, 32$ 를 사용했고, 패치의 개수는 다음과 같이 구한다.  

$$
N = \frac{H * W}{P^2}
$$  

예를 들어 이미지 사이즈가 (224 * 224)이고, $P = 16$ 이라면 $N = 196$ 이 되고, 이미지 분류를 위해 [CLS] 토큰을 패치의 가장 앞에 추가한다. 이 토큰은 랜덤한 값으로 초기화하고, 이미지의 전체 정보를 요약하여 최종 분류에 사용한다.  
또한, Transformer와 동일하게 Positional Embedding(PE)까지 더해주고나면 최종 입력이 된다.  

이를 수식으로 표현하면 다음과 같다.

$$
z_0 = [x_{cls}; x_p^1 E; \dots; x_p^N E] + E_{pos}
$$

$z$에 붙은 첨자는 몇 번째 레이어인지를 나타내고, 입력이기 때문에 0으로 표현됐다.  

그리고 이어서 $z_ℓ^′, z_ℓ, y$는 각각 아래와 같다.

![Encoder-1](/assets/2025-04-22-vit/2-2.png)  
_(Encoder에서 Attention 연산하는 부분)_  

$$
z_ℓ^′ = MSA(LN(z_{ℓ−1})) + z_{ℓ−1}
$$

![Encoder-2](/assets/2025-04-22-vit/2-3.png)  
_(Encoder에서 MLP 연산하는 부분)_  

$$
z_ℓ = MLP(LN(z_ℓ^′)) + z_ℓ^′
$$

$y$를 구하는 식에서 $z$에 붙은 첨자는 마지막 레이어에서 가장 첫 번째 패치의 값을 나타낸다. 가장 첫번째 패치는 위에서 말한 대로 [CLS]토큰의 자리이며, 최종적으로 분류를 위해 [CLS]토큰을 사용하게 되는 것이다.  

$$
y = LN(z_L^0)
$$

$$
\hat{y} = softmax(W_{head} z_L^0)
$$

---

## 4. Effect of Inductive Bias Differences on Model Performance

앞에서 transformer는 inductive bias가 약하고, CNN은 강하다고 했는데, ViT는 inductive bias 약해서 얻는 이점은 어떤 것이고, inductive bias가 약한 점을 어떻게 보완했는지 살펴보자.  

우선, CNN은 앞서 설명한 것처럼 **"Translation Equivariance, Hierarchical Structure, Locality"** 로 인해 Inductive Bias가 강하다. 즉, 비교적 적은 양의 데이터로 학습한다면 CNN이 더 높은 성능을 낼 확률이 높다. 다시 말해서 ViT가 CNN의 inductive bias 덕분에 얻을 수 있는 성능을 뛰어넘으려면 훨씬 더 많은 데이터가 필요하다.  

![cnn vs vit results](/assets/2025-04-22-vit/3.png)  

샘플 수가 10M 일 때, 가장 간단한 ResNet50은 가장 큰 모델인 ViT-L/16 보다도 좋은 성능을 보인다. 



---

## 🔗 Reference
[1] [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/pdf/2010.11929)