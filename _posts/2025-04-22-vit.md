---
title: "[논문 리뷰] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (ViT)"
date: 2025-04-22 14:00:00 +0900
categories: [Paper]
tags: [Transformer, Vision, ViT]
math: true
toc: true
---

## 📝 논문 정보

- **제목**: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale  
- **저자**: Dosovitskiy et al.  
- **학회/연도**: ICLR 2021  
- **링크**: [https://arxiv.org/pdf/2010.11929](https://arxiv.org/pdf/2010.11929)

---

## 1. Introduction

Transformer의 출시, 그리고 그 이후 관련 연구로 인해 Language 분야는 많은 발전을 이뤄왔다. 하지만 Vision 분야는 기존의 CNN구조를 벗어나지 못한 채 꽤 오랜 기간 정체되어있었다.  
이 논문은 CNN구조를 완전히 벗어난 Vision-Transformer(ViT)로 불리는 Transformer 기반 분류 방법을 제시했다. 이미지를 패치 단위로 쪼개서 각 패치를 단어, 이미지 전체를 문장인 것처럼 여기도록 접근했으며, Language 분야에서 높은 성능을 낸 것처럼 Vision 분야에서도 좋은 성능을 보이고 있다.  
이어서 CNN과의 차이, 왜 Transformer 구조가 성능이 잘 나오는지, Transformer 구조 덕분에 분류 성능 말고도 어떠한 장점이 있는지에 대해 설명하고자 한다.  

Transformer에 대한 자세한 내용은 이전에 리뷰한 내용을 먼저 보고 오는 것을 추천한다.  
[https://secretdsy.github.io/paper/transformer/](https://secretdsy.github.io/paper/transformer/)  

---

## 2. CNN vs Transformer

**이미지 분류에서 CNN이 잘 작동하는 이유는 뭘까?**  
CNN을 활용한 분류 과정을 살펴보면 다음과 같은 방식으로 작동한다.  

![CNN-architecture](/assets/2025-04-22-vit/1.png)  
_(CNN의 시초인 LeNet의 구조)_  

입력 -> **특징 추출을 위한 컨볼루션 연산** -> FC Layer -> 클래스 분류  

**특징 추출을 위한 컨볼루션 연산** 은 입력층에 가까울 수록 좁은 구역을 보고, 출력층으로 갈수록 더 넓은 영역을 보도록 설계되어있다.  
예를 들어 (224 * 224 * 3) 이미지를 입력으로하고 컨볼루션 필터 사이즈가 (3 * 3) 일 때, 첫 번째 레이어의 필터는 가로, 세로 3픽셀씩만 보게 된다. 그리고 두번째 레이어의 입력은 첫번째 레이어에서 특징을 추출한 것이고 보통은 가로, 세로 사이즈가 더 작아진다.  

![CNN-projection](/assets/2025-04-22-vit/1-2.png)  
_(두번째 컨볼루션 연산에서 얻은 feature map에서의 한 픽셀이 입력에서 차지하는 부분)_  

이렇게 작아진 입력에 똑같은 사이즈의 필터를 적용하면 원본에서 3픽셀이 아닌 더 많은 영역을 보는 것과 같다. 따라서 CNN은 레이어가 깊어질수록 더 넓은 영역을 보는 것처럼 작동한다.  

이러한 방식으로 인해 CNN은 "edge" 부분에 집중하고, 이후 레이어에서는  "texture 등 더 넓은 영역"에 집중하고, 최종적으로 "object" 전체를 보는 것과 같이 **계층적 구조(Hierarchical Structure)** 로 이루어져 있다.  

그리고 컨볼루션 연산은 **똑같은 필터(Weight Sharing)** 를 슬라이딩 윈도우 방식으로 이미지 전체를 훑어보고, 컨볼루션 연산 이후에 max pooling 또는 average pooling을 적용하기 때문에 픽셀이 이동하더라도 비슷한 특징을 추출한다. 이러한 방식 덕분에 CNN은 **이동 불변성(Translation Equivariance)** 이라는 특징을 갖게 된다.

또한, Transformer는 self-attention 연산을 통해 **"입력 전체"** 를 살펴보고 어느 부분을 더 **"Attend"** 할지를 정하도록 되어있다. 따라서 (N * N) 형태로 **국소적인 영역(Locality)** 을 훑고 다니는 컨볼루션 연산과 다르게 전체를 본다는 점이 가장 큰 차이다.  

앞에서 설명한 **"Translation Equivariance, Hierarchical Structure, Locality"** 로 인해 CNN은 Transformer에 비해 **Inductive Bias** 가 강하다고 표현한다.  

Transformer는 CNN에 비해 **Inductive Bias** 가 약한데 어떻게 성능을 더 높일 수 있었던건지, **Inductive Bias** 가 약해서 얻는 이점이 어떤건지는 이후에 설명하려한다.

---

## 3. ViT Architecture

![ViT-architecture](/assets/2025-04-22-vit/2.png)  
_(왼쪽: ViT 구조, 오른쪽: Transformer Encoder)_  

**이미지를 어떻게 입력받을까?**  
Transformer는 문장 전체, 즉 단어들을 입력받는다. ViT는 이미지를 여러 패치로 쪼개고, 각각의 패치를 단어처럼 취급하여 입력받는다.  

이 논문에서는 패치사이즈 $P = 16, 32$ 를 사용했고, 패치의 개수는 다음과 같이 구한다.  

$$
N = \frac{H * W}{P^2}
$$  

예를 들어 이미지 사이즈가 (224 * 224)이고, $P = 16$ 이라면 $N = 196$ 이 되고, 이미지 분류를 위해 **[CLS] 토큰**을 패치의 가장 앞에 추가한다. 이 토큰은 랜덤한 값으로 초기화하고, 이미지의 전체 정보를 요약하여 최종 분류에 사용한다.  
또한, Transformer와 동일하게 Positional Embedding(PE)까지 더해주고나면 최종 입력이 된다.  

이를 수식으로 표현하면 다음과 같다.  

$$
z_0 = [x_{cls}; x_p^1 E; \dots; x_p^N E] + E_{pos}
$$

$z$에 붙은 첨자는 몇 번째 레이어인지를 나타내고, 입력이기 때문에 0으로 표현됐다.  

그리고 이어서 $z_ℓ^′, z_ℓ, y$는 각각 아래와 같다.

![Encoder-1](/assets/2025-04-22-vit/2-2.png)  
_(Encoder에서 Attention 연산하는 부분)_  

$$
z_ℓ^′ = MSA(LN(z_{ℓ−1})) + z_{ℓ−1}
$$

![Encoder-2](/assets/2025-04-22-vit/2-3.png)  
_(Encoder에서 MLP 연산하는 부분)_  

$$
z_ℓ = MLP(LN(z_ℓ^′)) + z_ℓ^′
$$

$y$를 구하는 식에서 $z$에 붙은 첨자는 마지막 레이어에서 가장 첫 번째 패치의 값을 나타낸다. 가장 첫번째 패치는 위에서 말한 대로 [CLS]토큰의 자리이며, 최종적으로 분류를 위해 [CLS]토큰을 사용하게 되는 것이다.  

$$
y = LN(z_L^0)
$$

$$
\hat{y} = softmax(W_{head} z_L^0)
$$

---

## 4. Effect of Inductive Bias Differences on Model Performance

앞에서 transformer는 inductive bias가 약하고, CNN은 강하다고 했는데, ViT는 inductive bias가 약해서 얻는 이점은 어떤 것이고, inductive bias가 약한 점을 어떻게 보완했는지 살펴보자.  

우선, CNN은 앞서 설명한 것처럼 **"Translation Equivariance, Hierarchical Structure, Locality"** 로 인해 Inductive Bias가 강하다. 즉, 비교적 적은 양의 데이터로 학습한다면 CNN이 더 높은 성능을 낼 확률이 높다. 다시 말해서 ViT가 CNN의 inductive bias 덕분에 얻을 수 있는 성능을 뛰어넘으려면 훨씬 더 많은 데이터가 필요하다.  

![cnn vs vit results](/assets/2025-04-22-vit/3.png)  

샘플 수가 10M 일 때, 가장 간단한 ResNet50은 가장 큰 모델인 ViT-L/16 보다도 좋은 성능을 보이고, 300M일 때는 ViT-L/16이 가장 좋은 성능을 보인다.  

일반적으로 숫자가 더 클수록 크고 복잡한 모델인 것과는 반대로 ViT-L/16이 ViT-L/32보다 연산량이 더 많다. 왜냐하면 같은 해상도의 이미지에서 패치 사이즈가 작을 수록 더 많은 패치가 생성되어 attention 연산량이 증가하기 때문이다. 즉, transformer가 더 긴 문장을 입력 받는 것과 같다.

**ViT의 inductive bias가 약해서 얻을 수 있는 것은 뭘까?**  
지금까지 CNN의 inductive bias가 강해서 얻은 이점은 모두 **image classification**을 기반으로한 설명이었다. 그럼 다른 입력이나 다른 task에서는 어떨까?  
**결론부터 말하자면 ViT가 입력이나 학습 방식에 있어서 훨씬 더 유연하다.** Inductive bias가 강하다는 것은 특정 task에서는 아주 좋은 배경지식이 될 수 있지만, 반대로 그 task를 제외하고는 잘 못 할 수도 있는 것이다. CNN은 2D 이미지를 입력받고 특징을 압축해서 분류하는 것에 최적이고, transformer는 입력이 훨씬 자유롭다. 이 논문처럼 이미지가 될 수도 있고, 단어가 될 수도 있고, 심지어 동영상이 될 수도 있다.  
또한, **Trasnfer Learning**에 있어서 상당히 제한적이다. 이는 대부분의 CNN이 classification task에 대해 supervised learning 방식으로 pre-train 되었기 때문이다. 따라서 활용 가능성 자체가 classification과 같이 feature extraction이 포함된 task 정도로 제한된다. 즉, classification, detection에서는 활용이 가능하고 좋은 성능을 보일 수 있으나, zero-shot classification 같은 경우에는 성능이 좋지 않다.  
반대로 ViT는 학습 방식에 있어서 supervised learning, multimodal, self-supervised learning 등 상당히 유연하기 때문에 detection, segmentation, zero-shot classification과 같은 다양한 task에서 활용할 수 있다.  

결론적으로 transformer의 inductive bias가 약한 것은 많은 양의 데이터로 극복할 수 있으며, 충분한 양의 데이터가 주어진 환경에서 **전체 토큰간의 관계를 학습하는 transformer 방식이 더 좋은 성능을 낸다**.

---

## 5. Summary
ViT는 CNN 없이도 시각 인식이 가능하다는 것을 보여준 첫 논문이다. 심지어 가능성만 본 것이 아니라, 데이터가 충분한 상황에서는 더 좋은 성능을 보일 수 있음을 보여줬다.  
또한 **이미지를 패치 단위로 쪼개는 것과 transformer의 attention 구조**를 도입하여, 문장뿐만 아니라 이미지까지 입력이 가능하도록 한 덕분에 입출력이 유연하고, 그 결과 다양한 task로 확장할 수 있게 되었다.
Transformer가 Language 분야 및 AI 분야 전체에서 한 획을 그엇다면, ViT는 Transformer를 잘 활용하여 Vision 분야 전반에 미친 영향이 매우 크다.  

---

## 🔗 Reference
[1] [LeNet](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf)